name: Python CI/CD Workflow

on:
  push:
    branches:
      - main
      - spark
  pull_request:
    branches:
      - main
      - spark

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout Code
      - name: Checkout Code
        uses: actions/checkout@v3

      # Step 2: Cache Python Dependencies
      - name: Cache Python Dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # Step 3: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: 3.10.12

      # Step 4: Install Dependencies
      - name: Install Dependencies
        run: |
          set -e
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt

      # Step 5: Download Hadoop and AWS SDK Dependencies
      - name: Add Spark Hadoop and AWS SDK Dependencies
        run: |
          set -e
          mkdir -p ~/.ivy2/jars
          curl -L -o ~/.ivy2/jars/hadoop-aws-3.3.6.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar
          curl -L -o ~/.ivy2/jars/aws-java-sdk-bundle-1.12.526.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.526/aws-java-sdk-bundle-1.12.526.jar

      # Step 6: Set AWS Credentials
      - name: Set AWS Credentials
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_KEY }}
        run: |
          echo "AWS Credentials are set."

      # Step 7: Set PYTHONPATH
      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE/src" >> $GITHUB_ENV

      # Step 8: Train Model
      - name: Train Model
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_KEY }}
        run: |
          set -e
          source venv/bin/activate
          spark-submit \
            --packages org.apache.hadoop:hadoop-aws:3.3.6,com.amazonaws:aws-java-sdk-bundle:1.12.526 \
            src/train_rf_tuning.py

      # Step 9: Upload Model Artifacts
      - name: Upload Model Artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: model-artifacts
          path: models/tuned_rf_model

      # Step 10: Lint Code
      - name: Lint Code
        run: |
          set -e
          source venv/bin/activate
          pylint src tests

      # Step 11: Run Tests
      - name: Run Tests
        run: |
          set -e
          source venv/bin/activate
          pytest tests/ --junitxml=pytest-report.xml

      # Step 12: Upload Test Results
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: pytest-report
          path: pytest-report.xml
